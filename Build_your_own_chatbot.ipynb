{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2a6LEM_KvWR"
   },
   "source": [
    "# Building your own Chatbot from Scratch in Python (using NLTK)\n",
    "\n",
    "![Alt text](https://cdn-images-1.medium.com/max/800/1*pPcVfZ7i-gLMabUol3zezA.gif)\n",
    "\n",
    "History of chatbots dates back to 1966 when a computer program called ELIZA was invented by Weizenbaum. It imitated the language of a psychotherapist from only 200 lines of code. You can still converse with it here: [Eliza](http://psych.fullerton.edu/mbirnbaum/psych101/Eliza.htm?utm_source=ubisend.com&utm_medium=blog-link&utm_campaign=ubisend). \n",
    "\n",
    "On similar lines let's create a very basic chatbot utlising the Python's NLTK library.It's a very simple bot with hardly any cognitive skills,but still a good way to get into NLP and get to know about chatbots.\n",
    "\n",
    "For detailed analysis, please see the accompanying blog titled:**[Building a Simple Chatbot in Python (using NLTK](https://medium.com/analytics-vidhya/building-a-simple-chatbot-in-python-using-nltk-7c8c8215ac6e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nso6M6g0KvWU"
   },
   "source": [
    "## NLP\n",
    "NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPCNIpdJKvWX"
   },
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ytKb4HVAKvWZ"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qa8m-fe8KvWr"
   },
   "source": [
    "## Downloading and installing NLTK\n",
    "NLTK(Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n",
    "\n",
    "[Natural Language Processing with Python](http://www.nltk.org/book/) provides a practical introduction to programming for language processing.\n",
    "\n",
    "For platform-specific instructions, read [here](https://www.nltk.org/install.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "JRjPlOnTKvWt",
    "outputId": "02a32087-6b2e-44b9-9bc4-7c0cae94cd36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\kingsley\\anaconda3\\new anaconda\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\kingsley\\anaconda3\\new anaconda\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kingsley\\anaconda3\\new anaconda\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kingsley\\anaconda3\\new anaconda\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\kingsley\\anaconda3\\new anaconda\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kingsley\\anaconda3\\new anaconda\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsQ8TnrKKvW6"
   },
   "source": [
    "### Installing NLTK Packages\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ikyB47wnKvW8",
    "outputId": "edac40c9-af36-45af-849b-d5bf3a5be9c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kingsley\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kingsley\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular', quiet=True) # for downloading packages\n",
    "nltk.download('punkt') # first-time use only\n",
    "nltk.download('wordnet') # first-time use only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUFC9npYKvXF"
   },
   "source": [
    "## Reading in the corpus\n",
    "\n",
    "For our example,we will be using the Wikipedia page for chatbots as our corpus. Copy the contents from the page and place it in a text file named ‘chatbot.txt’. However, you can use any corpus of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "jCBu_xdpKvXH",
    "outputId": "62796170-e0c0-4234-f8e4-f9ea81f722e3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'articles for traditional encyclopedias such as encyclopædia britannica are written by experts, lending such encyclopedias a reputation for accuracy.[187] however, a peer review in 2005 of forty-two scientific entries on both wikipedia and encyclopædia britannica by the science journal nature found few differences in accuracy, and concluded that \"the average science entry in wikipedia contained around four inaccuracies; britannica, about three.\"[188] joseph reagle suggested that while the study reflects \"a topical strength of wikipedia contributors\" in science articles, \"wikipedia may not have fared so well using a random sampling of articles or on humanities subjects.\"[189] others raised similar critiques.[190] the findings by nature were disputed by encyclopædia britannica,[191][192] and in response, nature gave a rebuttal of the points raised by britannica.[193] in addition to the point-for-point disagreement between these two parties, others have examined the sample size and selection method used in the nature effort, and suggested a \"flawed study design\" (in nature\\'s manual selection of articles, in part or in whole, for comparison), absence of statistical analysis (e.g., of reported confidence intervals), and a lack of study \"statistical power\" (i.e., owing to small sample size, 42 or 4 × 101 articles compared, vs >105 and >106 set sizes for britannica and the english wikipedia, respectively).[194]\\n\\nas a consequence of the open structure, wikipedia \"makes no guarantee of validity\" of its content, since no one is ultimately responsible for any claims appearing in it.[195] concerns have been raised by pc world in 2009 regarding the lack of accountability that results from users\\' anonymity,[196] the insertion of false information,[197] vandalism, and similar problems.\\n\\neconomist tyler cowen wrote: \"if i had to guess whether wikipedia or the median refereed journal article on economics was more likely to be true after a not so long think i would opt for wikipedia.\" he comments that some traditional sources of non-fiction suffer from systemic biases, and novel results, in his opinion, are over-reported in journal articles as well as relevant information being omitted from news reports. however, he also cautions that errors are frequently found on internet sites and that academics and experts must be vigilant in correcting them.[198] amy bruckman has argued that, due to the number of reviewers, \"the content of a popular wikipedia page is actually the most reliable form of information ever created\".[199] in september 2022, the sydney morning herald journalist liam mannix noted that, \"there’s no reason to expect wikipedia to be accurate... and yet it [is].\" mannix further discussed the multiple studies that have proved wikipedia to be generally as reliable as encyclopedia britannica, sumarrizing that, \"...turning our back on such an extraordinary resource is… well, a little petty.\"[200]\\n\\ncritics argue that wikipedia\\'s open nature and a lack of proper sources for most of the information makes it unreliable.[201] some commentators suggest that wikipedia may be reliable, but that the reliability of any given article is not clear.[202] editors of traditional reference works such as the encyclopædia britannica have questioned the project\\'s utility and status as an encyclopedia.[203] wikipedia co-founder jimmy wales has claimed that wikipedia has largely avoided the problem of \"fake news\" because the wikipedia community regularly debates the quality of sources in articles.[204]\\n\\nexternal video\\nvideo icon inside wikipedia – attack of the pr industry, deutsche welle, 7:13 mins[205]\\nwikipedia\\'s open structure inherently makes it an easy target for internet trolls, spammers, and various forms of paid advocacy seen as counterproductive to the maintenance of a neutral and verifiable online encyclopedia.[93][206] in response to paid advocacy editing and undisclosed editing issues, wikipedia was reported in an article in the wall street journal to have strengthened its rules and laws against undisclosed editing.[207] the article stated that: \"beginning monday [from the date of the article, june 16, 2014], changes in wikipedia\\'s terms of use will require anyone paid to edit articles to disclose that arrangement. katherine maher, the nonprofit wikimedia foundation\\'s chief communications officer, said the changes address a sentiment among volunteer editors that, \\'we\\'re not an advertising service; we\\'re an encyclopedia.\\'\"[207][208][209][210][211] these issues, among others, had been parodied since the first decade of wikipedia, notably by stephen colbert on the colbert report.[212]\\n\\nlegal research in a nutshell (2011), cites wikipedia as a \"general source\" that \"can be a real boon\" in \"coming up to speed in the law governing a situation\" and, \"while not authoritative, can provide basic facts as well as leads to more in-depth resources\".[213]\\n\\ndiscouragement in education\\nsome university lecturers discourage students from citing any encyclopedia in academic work, preferring primary sources;[214] some specifically prohibit wikipedia citations.[215][216] wales stresses that encyclopedias of any type are not usually appropriate to use as citable sources, and should not be relied upon as authoritative.[217] wales once (2006 or earlier) said he receives about ten emails weekly from students saying they got failing grades on papers because they cited wikipedia; he told the students they got what they deserved. \"for god\\'s sake, you\\'re in college; don\\'t cite the encyclopedia\", he said.[218][219]\\n\\nin february 2007, an article in the harvard crimson newspaper reported that a few of the professors at harvard university were including wikipedia articles in their syllabi, although without realizing the articles might change.[220] in june 2007, former president of the american library association michael gorman condemned wikipedia, along with google, stating that academics who endorse the use of wikipedia are \"the intellectual equivalent of a dietitian who recommends a steady diet of big macs with everything\".[221]\\n\\ncontrarily, a 2016 article in the universal journal of educational research argued that \"wikipedia can be used for serious student projects...\" and that wikipedia is a good place to learn academic writing styles.[222] a 2020 research study published in studies in higher education argued that wikipedia could be applied in the higher education \"flipped classroom\", an educational model where students learn before coming to class and apply it in classroom activities. the experimental group was instructed to learn before class and get immediate feedback before going in (the flipped classroom model), while the control group was given direct instructions in class (the conventional classroom model). the groups were then instructed to collaboratively develop wikipedia entries, which would be graded in quality after the study. the results showed that the experimental group yielded more wikipedia entries and received higher grades in quality. the study concluded that learning with wikipedia in flipped classrooms was more effective than in conventional classrooms, proving that wikipedia could be used as an educational tool in higher education'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open('text_data.txt','r', errors=\"ignore\")\n",
    "raw=f.read() #read file\n",
    "raw = raw.lower()# converts to lowercase\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQMbWKz2KvXP"
   },
   "source": [
    "\n",
    "The main issue with text data is that it is all in text format (strings). However, the Machine learning algorithms need some sort of numerical feature vector in order to perform the task. So before we start with any NLP project we need to pre-process it to make it ideal for working. Basic text pre-processing includes:\n",
    "\n",
    "* Converting the entire text into **uppercase** or **lowercase**, so that the algorithm does not treat the same words in different cases as different\n",
    "\n",
    "* **Tokenization**: Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n",
    "\n",
    "_The NLTK data package includes a pre-trained Punkt tokenizer for English._\n",
    "\n",
    "* Removing **Noise** i.e everything that isn’t in a standard number or letter.\n",
    "* Removing the **Stop words**. Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words\n",
    "* **Stemming**: Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form — generally a written word form. Example if we were to stem the following words: “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, the result would be a single word “stem”.\n",
    "* **Lemmatization**: A slight variant of stemming is lemmatization. The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. Examples of Lemmatization are that “run” is a base form for words like “running” or “ran” or that the word “better” and “good” are in the same lemma so they are considered the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOWyEgMdKvXQ"
   },
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpjjYkZNKvXS"
   },
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)     # converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw) # converts to list of words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYWPNkqjKvXa"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "We shall now define a function called LemTokens which will take as input the tokens and return normalized tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OnKw1QQKvXb"
   },
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cMGjW5eKvXh"
   },
   "source": [
    "## Keyword matching\n",
    "\n",
    "Next, we shall define a function for a greeting by the bot i.e if a user’s input is a greeting, the bot shall return a greeting response.ELIZA uses a simple keyword matching for greetings. We will utilize the same concept here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4MchcWHKvXi"
   },
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():  #split sentence\n",
    "        if word.lower() in GREETING_INPUTS:   #get greeting inputs\n",
    "            return random.choice(GREETING_RESPONSES)  #get greeting responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVPfiFSdWlnK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5s2xQt9KvXq"
   },
   "source": [
    "## Generating Response\n",
    "\n",
    "### Bag of Words\n",
    "After the initial preprocessing phase, we need to transform text into a meaningful vector (or array) of numbers. The bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "* A vocabulary of known words.\n",
    "\n",
    "* A measure of the presence of known words.\n",
    "\n",
    "Why is it is called a “bag” of words? That is because any information about the order or structure of words in the document is discarded and the model is only **concerned with whether the known words occur in the document, not where they occur in the document.**\n",
    "\n",
    "The intuition behind the Bag of Words is that documents are similar if they have similar content. Also, we can learn something about the meaning of the document from its content alone.\n",
    "\n",
    "For example, if our dictionary contains the words {Learning, is, the, not, great}, and we want to vectorize the text “Learning is great”, we would have the following vector: (1, 1, 0, 0, 1).\n",
    "\n",
    "\n",
    "### TF-IDF Approach\n",
    "A problem with the Bag of Words approach is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much “informational content”. Also, it will give more weight to longer documents than shorter documents.\n",
    "\n",
    "One approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like “the” that are also frequent across all documents are penalized. This approach to scoring is called Term Frequency-Inverse Document Frequency, or TF-IDF for short, where:\n",
    "\n",
    "**Term Frequency: is a scoring of the frequency of the word in the current document.**\n",
    "\n",
    "```\n",
    "TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "```\n",
    "\n",
    "**Inverse Document Frequency: is a scoring of how rare the word is across documents.**\n",
    "\n",
    "```\n",
    "IDF = 1+log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "```\n",
    "### Cosine Similarity\n",
    "\n",
    "Tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus\n",
    "\n",
    "```\n",
    "Cosine Similarity (d1, d2) =  Dot product(d1, d2) / ||d1|| * ||d2||\n",
    "```\n",
    "where d1,d2 are two non zero vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVj3j4L2KvXr"
   },
   "source": [
    "To generate a response from our bot for input questions, the concept of document similarity will be used. We define a function response which searches the user’s utterance for one or more known keywords and returns one of several possible responses. If it doesn’t find the input matching any of the keywords, it returns a response:” I am sorry! I don’t understand you”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EFlU19DKvXt"
   },
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()   #sort flat\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdYVNkltKvXz"
   },
   "source": [
    "Finally, we will feed the lines that we want our bot to say while starting and ending a conversation depending upon user’s input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVcXxv1xKvX1",
    "outputId": "2346d2be-b980-4485-9fd8-8ba87b9d540e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
      "hi\n",
      "ROBO: fhey\n",
      "what's a chatbot\n",
      "ROBO: \n",
      "I am sorry! I don't understand you\n",
      "how does wiki work\n",
      "ROBO: \n",
      "[202] editors of traditional reference works such as the encyclopædia britannica have questioned the project's utility and status as an encyclopedia.\n",
      "who owns wiki\n",
      "ROBO: \n",
      "I am sorry! I don't understand you\n",
      "bye\n",
      "ROBO: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False   \n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(f\"ROBO: f{greeting(user_response)}\")\n",
    "            else:\n",
    "                print(\"ROBO: \")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXPOU5IyRtI4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
